{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 19:30:18.077429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-05 19:30:18.077450: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras.losses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydicom import dcmread\n",
    "import pydicom.pixel_data_handlers\n",
    "import nibabel as nib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from keras.layers import CategoryEncoding, IntegerLookup\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "CURRENT_DIR_PATH = os.getcwd()\n",
    "\n",
    "TRAIN_IMAGE_PATH = os.path.join(CURRENT_DIR_PATH, \"train_images\")\n",
    "SEGMENTATIONS_PATH = os.path.join(CURRENT_DIR_PATH, \"segmentations\")\n",
    "TRAIN_CSV_PATH = os.path.join(CURRENT_DIR_PATH, \"train.csv\")\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "batch_size = 75"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 19:30:26.048687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-05 19:30:26.048710: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-05 19:30:26.048726: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cbarb15-desktop): /proc/driver/nvidia/version does not exist\n",
      "2022-10-05 19:30:26.049032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "def pad_list(labels_to_pad):\n",
    "    return [labels_to_pad[len(labels_to_pad) - 1] if i >= len(labels_to_pad) else labels_to_pad[i] for i in range(0, 8)]\n",
    "\n",
    "def create_categories(labels_for_categories):\n",
    "    vocab = [1, 2, 3, 4, 5, 6, 7]\n",
    "    index = IntegerLookup(vocabulary=vocab)\n",
    "    encoder = CategoryEncoding(num_tokens=index.vocabulary_size(), output_mode='multi_hot')\n",
    "    categories = encoder(index(labels_for_categories))\n",
    "\n",
    "    return categories\n",
    "\n",
    "def decode_img(path):\n",
    "    with open(path, 'rb') as image_file:\n",
    "        ds = dcmread(image_file)\n",
    "        img = ds.pixel_array\n",
    "        img = img / 255\n",
    "        img = tf.reshape(img, [512, 512, 1])\n",
    "\n",
    "    return img\n",
    "\n",
    "def labels_for_image(vertebrae):\n",
    "    vertebrae_for_image = list(filter(lambda vertebra: vertebra > 0, vertebrae))\n",
    "    labels_to_pad = list((map(lambda vertebra: int(vertebra), vertebrae_for_image))) if len(vertebrae_for_image) > 0 else [0]\n",
    "    padded_labels = pad_list(labels_to_pad)\n",
    "    return create_categories(padded_labels)\n",
    "\n",
    "def preprocess_data():\n",
    "    segmentation_patient_files = os.listdir(SEGMENTATIONS_PATH)\n",
    "    train_images = []\n",
    "    train_image_labels = []\n",
    "\n",
    "    for segmentation_patient_file in segmentation_patient_files:\n",
    "        file_path = os.path.join(SEGMENTATIONS_PATH, segmentation_patient_file)\n",
    "        segmentation_file = nib.load(file_path).get_fdata()\n",
    "        segmentation_file_transposed = segmentation_file[:, ::-1, ::-1].transpose(2, 1, 0)\n",
    "\n",
    "        for slice_number in range(0, len(segmentation_file_transposed)):\n",
    "            dicom_slice = segmentation_file_transposed[slice_number]\n",
    "            vertebrae = np.unique(dicom_slice)\n",
    "            labels = labels_for_image(vertebrae)\n",
    "            train_image_labels.append(labels)\n",
    "            train_images_path = os.path.join(TRAIN_IMAGE_PATH, file_path.split(\"/\")[-1].replace('.nii', \"\"))\n",
    "            image_dir = os.listdir(train_images_path)\n",
    "            image_dir.sort(key=natural_keys)\n",
    "            image_path = os.path.join(train_images_path, image_dir[slice_number])\n",
    "            decoded_image = decode_img(image_path)\n",
    "            train_images.append(decoded_image)\n",
    "\n",
    "     # categories = create_categories(train_image_labels)\n",
    "    dataset_size = len(train_images)\n",
    "    val_size = int(dataset_size * 0.2)\n",
    "    x_t = train_images[val_size:]\n",
    "    y_t = train_image_labels[val_size:]\n",
    "\n",
    "    x_v = train_images[:val_size]\n",
    "    y_v = train_image_labels[:val_size]\n",
    "\n",
    "    return x_t, y_t, x_v, y_v\n",
    "\n",
    "x_train, y_train, x_val, y_val = preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 19:30:43.313489: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1417674752 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(filepaths, labels, is_training=True):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size=len(filepaths))\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(x_train, y_train)\n",
    "val_ds = create_dataset(x_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 19:28:14.148396: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1417674752 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature [[[[1.56862745e-02]\n",
      "   [3.92156863e-03]\n",
      "   [1.17647059e-02]\n",
      "   ...\n",
      "   [3.92156863e-03]\n",
      "   [7.84313725e-03]\n",
      "   [2.15686275e-01]]\n",
      "\n",
      "  [[9.41176471e-02]\n",
      "   [7.45098039e-02]\n",
      "   [0.00000000e+00]\n",
      "   ...\n",
      "   [1.17647059e-01]\n",
      "   [2.74509804e-02]\n",
      "   [2.19607843e-01]]\n",
      "\n",
      "  [[7.05882353e-02]\n",
      "   [1.13725490e-01]\n",
      "   [7.05882353e-02]\n",
      "   ...\n",
      "   [9.41176471e-02]\n",
      "   [6.27450980e-02]\n",
      "   [1.76470588e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.17647059e-01]\n",
      "   [4.31372549e-01]\n",
      "   [3.76470588e-01]\n",
      "   ...\n",
      "   [7.72549020e-01]\n",
      "   [6.74509804e-01]\n",
      "   [6.00000000e-01]]\n",
      "\n",
      "  [[2.94117647e-01]\n",
      "   [1.41176471e-01]\n",
      "   [1.13725490e-01]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]]\n",
      "\n",
      "  [[5.88235294e-02]\n",
      "   [2.35294118e-02]\n",
      "   [2.62745098e-01]\n",
      "   ...\n",
      "   [1.37254902e-01]\n",
      "   [7.84313725e-02]\n",
      "   [1.76470588e-01]]]\n",
      "\n",
      "\n",
      " [[[3.13725490e-01]\n",
      "   [2.62745098e-01]\n",
      "   [5.49019608e-02]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [1.96078431e-02]\n",
      "   [1.17647059e-02]]\n",
      "\n",
      "  [[3.21568627e-01]\n",
      "   [9.80392157e-02]\n",
      "   [1.52941176e-01]\n",
      "   ...\n",
      "   [1.88235294e-01]\n",
      "   [5.88235294e-02]\n",
      "   [7.05882353e-02]]\n",
      "\n",
      "  [[2.50980392e-01]\n",
      "   [7.05882353e-02]\n",
      "   [7.84313725e-02]\n",
      "   ...\n",
      "   [3.96078431e-01]\n",
      "   [3.52941176e-02]\n",
      "   [3.92156863e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.01960784e-01]\n",
      "   [7.45098039e-02]\n",
      "   [9.80392157e-02]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]]\n",
      "\n",
      "  [[3.56862745e-01]\n",
      "   [9.41176471e-02]\n",
      "   [1.96078431e-01]\n",
      "   ...\n",
      "   [3.88235294e-01]\n",
      "   [2.62745098e-01]\n",
      "   [0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00]\n",
      "   [3.92156863e-03]\n",
      "   [7.45098039e-02]\n",
      "   ...\n",
      "   [5.21568627e-01]\n",
      "   [7.21568627e-01]\n",
      "   [6.54901961e-01]]]\n",
      "\n",
      "\n",
      " [[[3.92156863e-02]\n",
      "   [1.17647059e-02]\n",
      "   [7.45098039e-02]\n",
      "   ...\n",
      "   [1.33333333e-01]\n",
      "   [2.11764706e-01]\n",
      "   [6.66666667e-02]]\n",
      "\n",
      "  [[1.96078431e-02]\n",
      "   [1.64705882e-01]\n",
      "   [0.00000000e+00]\n",
      "   ...\n",
      "   [1.76470588e-01]\n",
      "   [2.35294118e-02]\n",
      "   [5.88235294e-02]]\n",
      "\n",
      "  [[1.96078431e-02]\n",
      "   [8.62745098e-02]\n",
      "   [2.35294118e-02]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [1.17647059e-02]\n",
      "   [2.39215686e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.52941176e-02]\n",
      "   [2.07843137e-01]\n",
      "   [2.86274510e-01]\n",
      "   ...\n",
      "   [3.78823529e+00]\n",
      "   [3.76470588e+00]\n",
      "   [3.76862745e+00]]\n",
      "\n",
      "  [[2.31372549e-01]\n",
      "   [2.62745098e-01]\n",
      "   [4.70588235e-02]\n",
      "   ...\n",
      "   [2.42352941e+00]\n",
      "   [2.45098039e+00]\n",
      "   [2.45490196e+00]]\n",
      "\n",
      "  [[2.43137255e-01]\n",
      "   [3.13725490e-02]\n",
      "   [3.92156863e-03]\n",
      "   ...\n",
      "   [6.50980392e-01]\n",
      "   [4.11764706e-01]\n",
      "   [4.43137255e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[5.49019608e-02]\n",
      "   [2.98039216e-01]\n",
      "   [3.84313725e-01]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]\n",
      "   [1.33333333e-01]]\n",
      "\n",
      "  [[1.13725490e-01]\n",
      "   [1.56862745e-02]\n",
      "   [3.60784314e-01]\n",
      "   ...\n",
      "   [7.84313725e-03]\n",
      "   [6.66666667e-02]\n",
      "   [5.88235294e-02]]\n",
      "\n",
      "  [[4.70588235e-02]\n",
      "   [7.84313725e-03]\n",
      "   [7.84313725e-03]\n",
      "   ...\n",
      "   [4.70588235e-02]\n",
      "   [0.00000000e+00]\n",
      "   [5.88235294e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.17647059e-02]\n",
      "   [1.17647059e-02]\n",
      "   [1.96078431e-02]\n",
      "   ...\n",
      "   [2.00000000e-01]\n",
      "   [1.37254902e-01]\n",
      "   [9.01960784e-02]]\n",
      "\n",
      "  [[0.00000000e+00]\n",
      "   [7.84313725e-03]\n",
      "   [1.13725490e-01]\n",
      "   ...\n",
      "   [4.27450980e-01]\n",
      "   [8.23529412e-02]\n",
      "   [1.60784314e-01]]\n",
      "\n",
      "  [[0.00000000e+00]\n",
      "   [0.00000000e+00]\n",
      "   [2.47058824e-01]\n",
      "   ...\n",
      "   [3.68627451e-01]\n",
      "   [1.76470588e-01]\n",
      "   [4.31372549e-02]]]\n",
      "\n",
      "\n",
      " [[[3.17647059e-01]\n",
      "   [7.05882353e-02]\n",
      "   [5.49019608e-02]\n",
      "   ...\n",
      "   [8.23529412e-02]\n",
      "   [0.00000000e+00]\n",
      "   [3.60784314e-01]]\n",
      "\n",
      "  [[2.98039216e-01]\n",
      "   [2.86274510e-01]\n",
      "   [3.13725490e-02]\n",
      "   ...\n",
      "   [3.92156863e-02]\n",
      "   [2.27450980e-01]\n",
      "   [1.68627451e-01]]\n",
      "\n",
      "  [[9.01960784e-02]\n",
      "   [1.76470588e-01]\n",
      "   [1.60784314e-01]\n",
      "   ...\n",
      "   [2.39215686e-01]\n",
      "   [3.52941176e-02]\n",
      "   [1.56862745e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[7.25490196e-01]\n",
      "   [8.47058824e-01]\n",
      "   [8.23529412e-01]\n",
      "   ...\n",
      "   [3.54509804e+00]\n",
      "   [4.01568627e+00]\n",
      "   [4.29411765e+00]]\n",
      "\n",
      "  [[8.23529412e-02]\n",
      "   [2.07843137e-01]\n",
      "   [8.62745098e-02]\n",
      "   ...\n",
      "   [3.81960784e+00]\n",
      "   [3.35294118e+00]\n",
      "   [3.48235294e+00]]\n",
      "\n",
      "  [[1.17647059e-01]\n",
      "   [1.56862745e-01]\n",
      "   [0.00000000e+00]\n",
      "   ...\n",
      "   [3.70980392e+00]\n",
      "   [3.16078431e+00]\n",
      "   [2.56862745e+00]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00]\n",
      "   [9.41176471e-02]\n",
      "   [8.23529412e-02]\n",
      "   ...\n",
      "   [2.35294118e-02]\n",
      "   [1.17647059e-02]\n",
      "   [5.49019608e-02]]\n",
      "\n",
      "  [[1.17647059e-02]\n",
      "   [2.74509804e-02]\n",
      "   [4.31372549e-02]\n",
      "   ...\n",
      "   [9.80392157e-02]\n",
      "   [1.52941176e-01]\n",
      "   [2.54901961e-01]]\n",
      "\n",
      "  [[0.00000000e+00]\n",
      "   [1.64705882e-01]\n",
      "   [8.62745098e-02]\n",
      "   ...\n",
      "   [1.68627451e-01]\n",
      "   [2.27450980e-01]\n",
      "   [1.84313725e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.31372549e-01]\n",
      "   [3.92156863e-02]\n",
      "   [7.45098039e-02]\n",
      "   ...\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]\n",
      "   [0.00000000e+00]]\n",
      "\n",
      "  [[1.01960784e-01]\n",
      "   [7.05882353e-02]\n",
      "   [1.17647059e-01]\n",
      "   ...\n",
      "   [5.88235294e-02]\n",
      "   [3.84313725e-01]\n",
      "   [4.27450980e-01]]\n",
      "\n",
      "  [[3.52941176e-01]\n",
      "   [2.90196078e-01]\n",
      "   [8.23529412e-02]\n",
      "   ...\n",
      "   [6.47058824e-01]\n",
      "   [5.64705882e-01]\n",
      "   [7.01960784e-01]]]]\n",
      "Feature shape (75, 512, 512, 1)\n",
      "Label [[0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0.]]\n",
      "Label shape (75, 8)\n"
     ]
    }
   ],
   "source": [
    "for feature, label in train_ds.take(1):\n",
    "    print(f'Feature {feature}')\n",
    "    print(f'Feature shape {feature.shape}')\n",
    "    print(f'Label {label}')\n",
    "    print(f'Label shape {label.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 512, 512, 64)      3200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 256, 256, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 128, 128, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 256)     295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 64, 64, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1048576)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               134217856 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,336,520\n",
      "Trainable params: 135,336,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "import keras.layers as layers\n",
    "from tensorflow import keras\n",
    "\n",
    "def create_model():\n",
    "    seq_model = Sequential([\n",
    "        layers.Conv2D(64, 7, padding='same', activation='relu', input_shape=(512, 512, 1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(8, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return seq_model\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# model.summary()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbarb15/PycharmProjects/venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    ")\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "#     start_time = time.time()\n",
    "#\n",
    "#     for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             logits = model(x_batch_train, training=True)\n",
    "#             loss_value = loss_fn(y_batch_train, logits)\n",
    "#\n",
    "#         grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "#\n",
    "#         if step % 200 == 0:\n",
    "#             print(\n",
    "#                 \"Training loss (for one batch) at step %d: %.4f\"\n",
    "#                 % (step, float(loss_value))\n",
    "#             )\n",
    "#             print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x, training=training)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# l = loss(model, features, labels, training=False)\n",
    "# print(\"Loss test: {}\".format(l))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#  Loss function for training on breaks\n",
    "\n",
    "def get_weight(key, value):\n",
    "    is_vertebrae = re.match(r'^C[1-9]$', key)\n",
    "    if is_vertebrae and value == 0:\n",
    "        return 1\n",
    "    elif is_vertebrae and value == 1:\n",
    "        return 2\n",
    "    elif key == 'patient_overall' and value == 0:\n",
    "        return 7\n",
    "    elif key == 'patient_overall' and value == 1:\n",
    "        return 14\n",
    "\n",
    "def loss(patient_id):\n",
    "    df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    patient_row = df.loc[df['StudyInstanceUID'] == patient_id]\n",
    "    total_loss = 0\n",
    "    for i in range(1, patient_row.shape[1]):\n",
    "        key = patient_row.iloc[:, i].name\n",
    "        y_ij = patient_row.iloc[:, i].values[0]\n",
    "        w_j = get_weight(key, y_ij)\n",
    "        p_ij = random.uniform(0, 1)\n",
    "        total_loss += w_j * (y_ij * np.log(p_ij) + (1 - y_ij) * np.log(1 - p_ij))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "loss('1.2.826.0.1.3680043.780')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = os.path.join(CURRENT_DIR_PATH, \"flower_photos/daisy\")\n",
    "list_ds = tf.data.Dataset.list_files(str(f'{path}/*.jpg'))\n",
    "\n",
    "def parse_image(filename):\n",
    "  parts = tf.strings.split(filename, os.sep)\n",
    "  label = parts[-2]\n",
    "\n",
    "  image = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(image)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [128, 128])\n",
    "  return image, label\n",
    "\n",
    "# file_path = next(iter(list_ds))\n",
    "# image, label = parse_image(file_path)\n",
    "\n",
    "def show(image, label):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(label.numpy().decode('utf-8'))\n",
    "  plt.axis('off')\n",
    "\n",
    "images_ds = list_ds.map(parse_image)\n",
    "for image, label in images_ds.take(2):\n",
    "    show(image, label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
    "# def upper_case_fn(t: np.ndarray):\n",
    "#   return t.decode('utf-8').upper()\n",
    "# # d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
    "# #           inp=[x], Tout=tf.string))\n",
    "# d = d.map(lambda x: upper_case_fn(x))\n",
    "# list(d.as_numpy_iterator())\n",
    "\n",
    "# def add_one(x):\n",
    "#     return x * 200\n",
    "#\n",
    "# dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "# dataset = dataset.map(add_one)\n",
    "# list(dataset.as_numpy_iterator())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}